import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from urllib.parse import urljoin
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# Nastaven√≠ hlaviƒçek pro simulaci po≈æadavku od skuteƒçn√©ho prohl√≠≈æeƒçe (pom√°h√° to p≈ôedej√≠t blokov√°n√≠ webem)
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'Accept-Language': 'cs-CZ,cs;q=0.9'
}

# Seznam URL pro r≈Øzn√© kategorie aut
categories = [
    "https://www.aaaauto.cz/sleva/",
    "https://www.aaaauto.cz/4x4-offroad-suv/",
    "https://www.aaaauto.cz/luxusni-vozy/",
]

def create_session():
    """
    Vytvo≈ô√≠ a vr√°t√≠ objekt session pro odes√≠l√°n√≠ HTTP po≈æadavk≈Ø. Konfiguruje politiku opakov√°n√≠ po≈æadavk≈Ø v p≈ô√≠padƒõ
    selh√°n√≠. Session je nakonfigurov√°na s politikou opakov√°n√≠, kter√° povol√≠ celkem 5 pokus≈Ø s exponenci√°ln√≠m zpo≈ædƒõn√≠m
    v p≈ô√≠padƒõ urƒçit√Ωch chyb serveru.
    """
    session = requests.Session()
    retries = Retry(
        total=5,  # Poƒçet pokus≈Ø o opakov√°n√≠
        backoff_factor=0.5,  # Zpo≈ædƒõn√≠ mezi pokusy roste exponenci√°lnƒõ
        status_forcelist=[429, 500, 502, 503, 504],  # Opakovat p≈ôi tƒõchto k√≥dech stavu
        allowed_methods=['GET']  # Povolit opakov√°n√≠ pouze pro GET po≈æadavky
    )
    session.mount('https://', HTTPAdapter(max_retries=retries))
    return session

def clean_text(text):
    """
    Vyƒçist√≠ vstupn√≠ text t√≠m, ≈æe odstran√≠ zbyteƒçn√© mezery a rozdƒõl√≠ text podle libovoln√©ho b√≠l√©ho znaku,
    n√°slednƒõ znovu spoj√≠. Pokud je text pr√°zdn√Ω nebo None, vr√°t√≠ None.
    """
    return ' '.join(str(text).strip().split()) if text else None

def extract_car_data(session, url):
    """
    Extrahuje data o autƒõ z dan√© URL. Odes√≠l√° HTTP po≈æadavek na str√°nku, analyzuje HTML a z√≠sk√° po≈æadovan√©
    informace o autƒõ, jako je kombinovan√° spot≈ôeba, rok uveden√≠ do provozu, typ karoserie, palivo, motor a v√Ωkon.
    """
    try:
        response = session.get(url, headers=headers, timeout=(5, 10))  # HTTP po≈æadavek
        soup = BeautifulSoup(response.text, 'html.parser')  # Parsov√°n√≠ HTML

        # Inicializace slovn√≠ku pro uchov√°n√≠ z√≠skan√Ωch dat
        data = {
            'Kombinovan√°': None,
            'Rok uveden√≠ do provozu': None,
            'Karoserie': None,
            'Palivo': None,
            'Motor': None,
            'V√Ωkon': None
        }

        # Proch√°zen√≠ seznamu 'li' element≈Ø a hled√°n√≠ pot≈ôebn√Ωch informac√≠
        for li in soup.find_all('li'):
            text = clean_text(li.get_text())  # Vyƒçi≈°tƒõn√≠ textu
            if not text:  # Pokud je text pr√°zdn√Ω, p≈ôeskoƒç√≠me
                continue
            strong = li.find('strong')  # Hled√°n√≠ tagu <strong> pro hodnotu
            value = clean_text(strong.text) if strong else None

            # Ulo≈æen√≠ hodnot podle kl√≠ƒçe (nap≈ô. Kombinovan√°, Rok uveden√≠ do provozu, ...)
            if 'Kombinovan√°' in text:
                data['Kombinovan√°'] = value
            elif 'Rok uveden√≠ do provozu' in text:
                data['Rok uveden√≠ do provozu'] = value
            elif 'Karoserie' in text:
                data['Karoserie'] = value
            elif 'Palivo' in text:
                data['Palivo'] = value
            elif 'Motor' in text:
                data['Motor'] = value
            elif 'V√Ωkon' in text:
                if value:
                    for part in value.split():  # Hled√°n√≠ v√Ωkonu ve form√°tu s 'kw'
                        if 'kw' in part.lower():
                            data['V√Ωkon'] = part.lower()
                            break

        # Pokud chyb√≠ kombinovan√° spot≈ôeba, zkus√≠me ji naj√≠t na jin√©m m√≠stƒõ na str√°nce
        if not data['Kombinovan√°']:
            spotreba_span = soup.find('span', class_='countbarValue')
            if spotreba_span:
                nested = spotreba_span.find('span')
                if nested:
                    value = clean_text(nested.get_text())
                    if value and 'l/100km' in value:
                        data['Kombinovan√°'] = value

        # Pokud jsou z√≠sk√°na nƒõjak√° data, vr√°t√≠me je, jinak vr√°t√≠me None
        return data if any(data.values()) else None
    except Exception as e:
        print(f"Chyba: {url} | {str(e)}")  # Pokud dojde k chybƒõ, vyp√≠≈°e chybu
        return None

def get_car_links(session, base_url):
    """
    Z√≠sk√° v≈°echny odkazy na auta z dan√© str√°nky kategorie. Proch√°zen√≠m HTML hled√° odkazy, kter√© obsahuj√≠ '/car.html'
    a maj√≠ parametr 'id='.
    """
    try:
        response = session.get(base_url, headers=headers, timeout=10)  # HTTP po≈æadavek
        soup = BeautifulSoup(response.text, 'html.parser')  # Parsov√°n√≠ HTML
        return list(set(
            urljoin(base_url, a['href'])  # Z√≠sk√°n√≠ √∫pln√Ωch URL
            for a in soup.select('a[href*="/car.html"]')  # V√Ωbƒõr odkaz≈Ø, kter√© obsahuj√≠ '/car.html'
            if 'id=' in a['href']  # Kontrola, ≈æe URL obsahuje parametr 'id='
        ))
    except Exception as e:
        print(f"Chyba p≈ôi z√≠sk√°v√°n√≠ odkaz≈Ø: {str(e)}")  # Chyba p≈ôi z√≠sk√°v√°n√≠ odkaz≈Ø
        return []

def process_category(session, category_url, all_results):
    """
    Zpracov√°v√° jednu kategorii aut, proch√°z√≠ jednotliv√© str√°nky (a≈æ do str√°nky 30) a z√≠sk√°v√° odkazy na auta.
    Pro ka≈æd√Ω odkaz na auto zavol√° funkci pro extrakci dat o autƒõ a ulo≈æ√≠ je do seznamu.
    """
    for page in range(1, 30):  # Proch√°z√≠ a≈æ 30 str√°nek
        if page == 1:
            url = category_url
        else:
            # Vytv√°≈ô√≠ URL pro dal≈°√≠ str√°nky podle kategorie
            if "sleva" in category_url:
                url = f"{category_url}#!&category=156&page={page}"
            elif "4x4-offroad-suv" in category_url:
                url = f"{category_url}#!&category=15&page={page}"
            elif "luxusni-vozy" in category_url:
                url = f"{category_url}#!&category=35&sort[]=0&sort[]=1&page={page}"
            else:
                url = f"{category_url}#!&page={page}"

        # Z√≠sk√°n√≠ odkaz≈Ø na auta z t√©to str√°nky
        car_links = get_car_links(session, url)
        if not car_links:  # Pokud nejsou ≈æ√°dn√© odkazy, ukonƒç√≠me zpracov√°n√≠
            break

        print(f"\nüîπ Str√°nka {page} ({len(car_links)} aut)")  # Vyp√≠≈°e poƒçet aut na str√°nce
        for i, link in enumerate(car_links, 1):
            try:
                time.sleep(random.expovariate(1.5))  # N√°hodn√© zpo≈ædƒõn√≠ pro zpomalen√≠ scrapov√°n√≠
                print(f"Zpracov√°v√°m {i}/{len(car_links)}: {link[:70]}...")  # Informace o zpracov√°van√©m odkazu
                car_data = extract_car_data(session, link)  # Extrahov√°n√≠ dat o autƒõ
                if car_data:
                    all_results.append(car_data)  # P≈ôid√°n√≠ dat do seznamu v√Ωsledk≈Ø
                    print("‚úÖ OK")
                else:
                    print("‚ùå Chyby ve zpracov√°n√≠")
            except KeyboardInterrupt:
                print("\nüõë Ukonƒçeno u≈æivatelem")  # Ukonƒçen√≠, pokud u≈æivatel p≈ôeru≈°√≠ proces
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è Chyba: {str(e)}")  # Vyp√≠≈°e chybu, pokud dojde k nƒõjak√© neoƒçek√°van√© chybƒõ
    return False

def save_results(all_results):
    """
    Ulo≈æ√≠ z√≠skan√° data o autech do CSV souboru. Pokud nejsou ≈æ√°dn√° data, vyp√≠≈°e chybovou zpr√°vu.
    """
    if all_results:
        df = pd.DataFrame(all_results)  # P≈ôevede seznam dat na DataFrame
        df.to_csv('vsechna_auta.csv', index=False, encoding='utf-8-sig')  # Ulo≈æ√≠ do CSV
        print(f"\n‚úÖ Ulo≈æeno {len(all_results)} z√°znam≈Ø do vsechna_auta.csv")
    else:
        print("\n‚ùå ≈Ω√°dn√° data k ulo≈æen√≠")

def main():
    """
    Hlavn√≠ funkce, kter√° spust√≠ cel√Ω scraping. Vytvo≈ô√≠ session, proch√°z√≠ v≈°echny kategorie aut
    a ukl√°d√° z√≠skan√° data.
    """
    session = create_session()  # Vytvo≈ôen√≠ session
    all_results = []  # Seznam pro uchov√°n√≠ v≈°ech v√Ωsledk≈Ø
    try:
        # Pro ka≈ædou kategorii aut zavol√° funkci pro zpracov√°n√≠
        for category_url in categories:
            print(f"\nüìÇ Zpracov√°v√°m kategorii: {category_url}")
            user_stopped = process_category(session, category_url, all_results)
            if user_stopped:  # Pokud u≈æivatel p≈ôeru≈°√≠, ukonƒç√≠me
                break
    except KeyboardInterrupt:
        print("\nüõë Ukonƒçeno u≈æivatelem")  # Ukonƒçen√≠ p≈ôi p≈ôeru≈°en√≠ u≈æivatelem
    finally:
        save_results(all_results)  # Ulo≈æen√≠ v√Ωsledk≈Ø
        session.close()  # Uzav≈ôen√≠ session

if __name__ == "__main__":
    main()  # Spu≈°tƒõn√≠ hlavn√≠ funkce
